Taylor T. Johnson

This repo includes two programs â€” a web crawler that scrapes links and words on pages starting from a specified root URL and a simple search engine server which uses
the data collected by the web crawler.
	
					webcrawler

This program takes in root URLs as command line args,
fetches the HTML files of the roots, finds links found
in said files, and creates an array as well as a linked list of 
these URLs leading to valid HTML or text files. While parsing the HTML
files, a description of each page is extracted from the first 500 characters
in the content. Words encountered in the content are mapped to a hash table
containing a linked list of all the URLs that lead to pages in which the
word can be found.

	Usage: webcrawler <url> <max urls>


					search_engine

This program uses the output from the web crawler (url.txt and words.txt) for use as a search engine. 

    Usage: search-engine port (array | hash | avl | bsearch) -r
      It starts a search engine at this port using the
      data structure indicated. Port has to be larger than 1024.
      Without the -r flag, only results containing all searched words
      Will be displayed. The -r flag specifies ranked result.
      Search results will be displayed in ranked order.

There are no known bugs with either my webcrawler, search-engine, or any of my dictionaries.
I did not implement AVL remove, however.

I have slightly altered what my webcrawler accepts as a valid "word." onContentFound() adds characters to a word buffere until an escape, period, comma, colon, semicolon, etc. is encountered. At that point, the buffer is checked to ensure that it contains only letters and apostrophes before the word is added.

Including a -r flag after specifying the dictionary type when starting the program will allow all hits for the searched words to display in ranked order, rather than only the pages that contain all of the words. 